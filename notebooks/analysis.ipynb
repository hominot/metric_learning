{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Setup\n",
    "\n",
    "- Let $\\mathcal{D}=\\{(x_i,y_i )\\}_{i=1}^N$  denote a training set of $N$ labeled examples where $x_i \\in \\mathcal{X}$ are images, $y_i \\in \\mathcal{Y}$ are discrete class labels\n",
    "- $f_\\theta : \\mathcal{X} \\rightarrow \\mathbb{R}^d$ is an embedding (nonlinear) function parametrized by $\\theta$\n",
    "- Goal is to find $f_\\theta$ that places embeddings of images of the same class labels close together.\n",
    "- Shorthand notation\n",
    "  - $f_i=f_\\theta (x_i )$\n",
    "  - $\\mathcal{N}=\\{1,…,N\\}$\n",
    "  - $\\langle g,h \\rangle =g^T h$ (dot product)\n",
    "- Datasets\n",
    "  - $\\mathcal{P} = \\{(i, j) \\in \\mathcal{N} \\times \\mathcal{N} : i \\neq j\\}$\n",
    "  - $\\mathcal{T}_3 =\\{(i, j, k) \\in \\mathcal{N}^3 :i \\neq j, y_i = y_j, y_i \\neq y_k\\}$\n",
    "  - $\\mathcal{T}_{n+1} = \\{(i,j,k_1,…,k_{n−1} ) \\in \\mathcal{N}^{n+1}: i \\neq j, y_i=y_j,y_i \\neq y_{k_1}, \\dots ,y_i \\neq y_{k_{n−1} } \\}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss Functions\n",
    "\n",
    "## Contrastive Loss\n",
    "\n",
    "- Original loss introduced in paper:\n",
    "$$\n",
    "\\mathcal{L}(\\mathcal{D};\\theta) = \\sum_{(i, j) \\in \\mathcal{P}} 1\\{y_i=y_j\\} \\Vert f_i − f_j \\Vert_2^2 + 1\\{y_i \\neq y_j\\} \\max⁡(0, m−\\Vert f_i−f_j \\Vert_2)^2\n",
    "$$\n",
    "\n",
    "- Modified loss for comparing with other losses\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathcal{L}(\\mathcal{D};\\theta) &= \\sum_{(i, j) \\in \\mathcal{P}} - 1\\{y_i=y_j\\} (m - \\Vert f_i − f_j \\Vert_2^2) + 1\\{y_i \\neq y_j\\} \\max⁡(0, m−\\Vert f_i−f_j \\Vert_2^2) \\\\\n",
    "&= \\sum_{(i, j) \\in \\mathcal{P}} - 1\\{y_i=y_j\\} S_1(x_i, x_j) + 1\\{y_i \\neq y_j\\} \\max⁡(0, S_1(x_i, x_j))\n",
    "\\end{align*}\n",
    "$$\n",
    "where $S_1(x_i, x_j) = m - \\Vert f_i - f_j \\Vert_2^2$ is a similarity score function.\n",
    "\n",
    "## Triplet Loss\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathcal{L}(\\mathcal{D};\\theta) &= \\sum_{(i,j,k)\\in \\mathcal{T}_3} \\max⁡(0, \\Vert f_i−f_j \\Vert_2^2 − \\Vert f_i−f_k \\Vert_2^2 + m) \\\\\n",
    "&= \\sum_{(i,j,k)\\in \\mathcal{T}_3} \\max⁡(0, S_1(x_i, x_k) − S_1(x_i, x_j) + m)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "## $(N+1)$-tuplet Loss\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathcal{L}(\\mathcal{D};\\theta) &= \\sum_{(i,j,k_1,\\dots,k_{n−1} ) \\in \\mathcal{T}_{n+1}} \\log⁡⁡\\left(1+\\sum_{l=1}^{n−1} \\exp\\left(\\langle f_i,f_{k_l} \\rangle − \\langle f_i,f_j \\rangle\\right) \\right) \\\\\n",
    "&= \\sum_{(i,j,k_1,\\dots,k_{n−1} ) \\in \\mathcal{T}_{n+1}} \\log⁡⁡\\left(1+\\sum_{l=1}^{n−1} \\exp\\left(S_2(x_i, x_{k_l}) − S_2(x_i, x_j) \\right) \\right) \\\\\n",
    "&= -\\sum_{(i,j,k_1,\\dots,k_{n−1} ) \\in \\mathcal{T}_{n+1}} \\log⁡⁡\\frac{\\exp(S_2(x_i, x_j))}{\\exp(S_2(x_i, x_j))+\\sum_{l=1}^{n−1} \\exp(S_2(x_i, x_{k_l}))}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "where $S_2(x_i, x_j) = \\langle x_i, x_j \\rangle$ is another similarity score function.\n",
    "\n",
    "## Random Graph Loss\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathcal{L}(\\mathcal{D};\\theta) &= \\sum_{(i, j) \\in \\mathcal{P}} \\log⁡(1+\\exp S_1(x_i, x_j))−1\\{y_i=y_j\\} S_1(x_i, x_j) \\\\\n",
    "&= -\\sum_{(i, j) \\in \\mathcal{P}} \\left[ 1\\{y_i=y_j\\} \\log \\frac{\\exp\\left(S_1(x_i, x_j)\\right)}{1+ \\exp\\left(S_1(x_i, x_j)\\right)} + 1\\{y_i \\neq y_j\\}\\log \\frac{1}{1+ \\exp\\left(S_1(x_i, x_j)\\right)} \\right]\\\\\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss Function Comparison\n",
    "\n",
    "In this section, we compare the $(n+1)$-tuplet loss function and the random graph loss function.\n",
    "\n",
    "- For comparison, use the Euclidean distance based similarity score $S_1$ for both loss functions.\n",
    "- Use the $(n+1)$-tuplet dataset $\\mathcal{T}_{n+1}$ for both loss functions. Note that the $(n+1)$-tuplet loss function uses $n$ pairwise similarity scores for each $(n+1)$ tuplet. The random graph loss function could use all $n(n+1)$ pairwise similarity scores for each tuplet. However, to isolate the effects of loss functions, use the same pairs used in the $(n+1)$-tuplet loss function.\n",
    "\n",
    "The following two are compared:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\mathcal{D}; \\theta) = -\\sum_{(i,j,k_1,\\dots,k_{n−1} ) \\in \\mathcal{T}_{n+1}} \\log⁡⁡\\frac{\\exp(S_1(x_i, x_j))}{\\exp(S_1(x_i, x_j))+\\sum_{l=1}^{n−1} \\exp(S_1(x_i, x_{k_l}))}\n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\mathcal{D}; \\theta) = -\\sum_{(i,j,k_1,\\dots,k_{n−1} ) \\in \\mathcal{T}_{n+1}} \\left[ \\log \\frac{\\exp\\left(S_1(x_i, x_j)\\right)}{1+ \\exp\\left(S_1(x_i, x_j)\\right)} + \\sum_{l=1}^{n-1} \\log \\frac{1}{1 + \\exp(S_1(x_i, x_{k_l}))} \\right]\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
